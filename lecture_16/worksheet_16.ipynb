{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "# Worksheet 16\n",
    "\n",
    "Name:  Esther Choi\n",
    "UID: U24585295\n",
    "\n",
    "### Topics\n",
    "\n",
    "- Support Vector Machines (Non-linear case)\n",
    "\n",
    "## Support Vector Machines\n",
    "\n",
    "Follow along in class to implement the perceptron algorithm and create an animation of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) As we saw in class, the form\n",
    "$$w^T x + b = 0$$\n",
    "while simple, does not expose the inner product `<x_i, x_j>` which we know `w` depends on, having done the math. This is critical to applying the \"kernel trick\" which allows for learning non-linear decision boundaries. Let's modify the above algorithm to use the form\n",
    "$$\\sum_i \\alpha_i <x_i, x> + b = 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets as datasets\n",
    "\n",
    "TEMPFILE = \"temp.png\"\n",
    "CENTERS = [[0, 1], [1, 0]]\n",
    "\n",
    "epochs = 100\n",
    "learning_rate = .05\n",
    "expanding_rate = .99\n",
    "retracting_rate = 1.1\n",
    "\n",
    "X, labels = datasets.make_blobs(n_samples=10, centers=CENTERS, cluster_std=0.2, random_state=0)\n",
    "Y = np.array(list(map(lambda x : -1 if x == 0 else 1, labels.tolist())))\n",
    "\n",
    "alpha_i = np.zeros((len(X),))\n",
    "b = 0\n",
    "\n",
    "def snap(x, alpha_i, b, error):\n",
    "    # create a mesh to plot in\n",
    "    h = .01  # step size in the mesh\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    meshData = np.c_[xx.ravel(), yy.ravel()]\n",
    "    cs = np.array([x for x in 'gb'])\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X[:,0],X[:,1],color=cs[labels].tolist(), s=50, alpha=0.8)\n",
    "\n",
    "    if error:\n",
    "        ax.add_patch(plt.Circle((x[0], x[1]), .12, color='r',fill=False))\n",
    "    else:\n",
    "        ax.add_patch(plt.Circle((x[0], x[1]), .12, color='y',fill=False))\n",
    "   \n",
    "    Z = predict_many(alpha_i, b, meshData)\n",
    "    Z = np.array([0 if z <=0 else 1 for z in Z]).reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, alpha=.5, cmap=plt.cm.Paired)\n",
    "    fig.savefig(TEMPFILE)\n",
    "    plt.close()\n",
    "    return im.fromarray(np.asarray(im.open(TEMPFILE)))\n",
    "\n",
    "def predict_many(alpha_i, b, Z):\n",
    "    res = []\n",
    "    for i in range(len(Z)):\n",
    "        res.append(predict(alpha_i, b, Z[i]))\n",
    "    return np.array(res)\n",
    "\n",
    "def predict(alpha_i, b, x):\n",
    "    return np.sign(np.sum(alpha_i * Y * np.dot(X, x)) + b)\n",
    "\n",
    "images = []\n",
    "for _ in range(epochs):\n",
    "    # pick a point from X at random\n",
    "    i = np.random.randint(0, len(X))\n",
    "    error = False\n",
    "    x, y = X[i], Y[i]\n",
    "    \n",
    "    if y * predict(alpha_i, b, x) <= 0:\n",
    "        alpha_i[i] += learning_rate\n",
    "        b += learning_rate * y\n",
    "        error = True\n",
    "        \n",
    "    images.append(snap(x, alpha_i, b, error))\n",
    "\n",
    "images[0].save(\n",
    "    'svm_dual.gif',\n",
    "    optimize=False,\n",
    "    save_all=True,\n",
    "    append_images=images[1:],\n",
    "    loop=0,\n",
    "    duration=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Write a configurable kernel function to apply in lieu of the dot product. Try it out on a dataset that is not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def polynomial(x_i, x_j, c, n):\n",
    "    return (np.dot(x_i, x_j) + c) ** n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gaussian_kernel(x_i, x_j, sigma=1):\n",
    "    diff = np.subtract(x_i, x_j)\n",
    "    squared_norm = np.dot(diff, diff)\n",
    "    return np.exp(-squared_norm / (2 * sigma ** 2))\n",
    "\n",
    "X = np.array([[0, 0], [1, 1], [2, 2], [3, 3]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "K = np.zeros((len(X), len(X)))\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(X)):\n",
    "        K[i, j] = gaussian_kernel(X[i], X[j], sigma=1)\n",
    "\n",
    "# Display kernel matrix\n",
    "print(\"Kernel Matrix:\")\n",
    "print(K)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Assume we fit an SVM using a polynomial Kernel function and it seems to overfit the data. How would you adjust the tuning parameter `n` of the kernel function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Decrease the value of n: Reduce the degree of the polynomial kernel. Lower values of n result in simpler decision boundaries, which are less likely to overfit the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Assume we fit an SVM using a RBF Kernel function and it seems to underfit the data. How would you adjust the tuning parameter `sigma` of the kernel function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "Increase the value of sigma: Increase the width of the Gaussian kernel. A larger sigma results in smoother decision boundaries, allowing the model to capture more complex patterns in the data. By increasing sigma, you allow the influence of each training point to extend further, leading to more flexible decision boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [],
    "user_expressions": []
   },
   "source": [
    "d) Tune the parameter of a specific Kernel function, to fit an SVM (using your code above) to the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "data = np.loadtxt(\"spiral.data\")\n",
    "x, y = data[:, :2], data[:, 2]\n",
    "\n",
    "plt.scatter(x[:,0], x[:,1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "K = np.zeros((len(x), len(x)))\n",
    "sigma = 0.5  # Adjust this value to tune sigma\n",
    "for i in range(len(x)):\n",
    "    for j in range(len(x)):\n",
    "        K[i, j] = gaussian_kernel(x[i], x[j], sigma=sigma)\n",
    "\n",
    "# Define SVM with Gaussian kernel\n",
    "svm = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm', SVC(kernel='precomputed'))  # Use 'precomputed' kernel\n",
    "])\n",
    "\n",
    "svm.fit(K, y)\n",
    "\n",
    "# Predict labels\n",
    "y_pred = svm.predict(K)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_min, x_max = x[:, 0].min() - 0.1, x[:, 0].max() + 0.1\n",
    "y_min, y_max = x[:, 1].min() - 0.1, x[:, 1].max() + 0.1\n",
    "xx, yy = np.meshgrid(np.linspace(x_min, x_max, 500),\n",
    "                     np.linspace(y_min, y_max, 500))\n",
    "Z = svm.predict(np.array([[gaussian_kernel(np.array([xi, yi]), np.array([xj, yj]), sigma=sigma) \n",
    "                           for xi, yi in zip(xx.ravel(), yy.ravel())] \n",
    "                          for xj, yj in x])).reshape(xx.shape)\n",
    "plt.contourf(xx, yy, Z, alpha=0.8)\n",
    "plt.scatter(x[:, 0], x[:, 1], c=y, edgecolors='k')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.title('SVM Decision Boundary (Gaussian Kernel)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "76ca05dc3ea24b2e3b98cdb7774adfbb40773424bf5109b477fd793f623715af"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
